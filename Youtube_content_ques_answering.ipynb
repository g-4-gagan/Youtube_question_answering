{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9954b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61af1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yt_dlp\n",
    "from pytube import YouTube\n",
    "from speechbrain.inference import EncoderDecoderASR\n",
    "import streamlit as st\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream_url(youtube_url):\n",
    "    ydl_opts = {\n",
    "        'quiet': True,\n",
    "        'format': 'best[ext=mp4]/best',\n",
    "    }\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info_dict = ydl.extract_info(youtube_url, download=False)\n",
    "        return info_dict['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1caac3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Szox9wD4HRU\n",
      "[youtube] Szox9wD4HRU: Downloading webpage\n",
      "[youtube] Szox9wD4HRU: Downloading tv client config\n",
      "[youtube] Szox9wD4HRU: Downloading player 461f4c95-main\n",
      "[youtube] Szox9wD4HRU: Downloading tv player API JSON\n",
      "[youtube] Szox9wD4HRU: Downloading ios player API JSON\n",
      "[youtube] Szox9wD4HRU: Downloading m3u8 information\n",
      "Playing audio...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlaying audio...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m sd\u001b[38;5;241m.\u001b[39mplay(samples, samplerate\u001b[38;5;241m=\u001b[39maudio\u001b[38;5;241m.\u001b[39mframe_rate)\n\u001b[0;32m---> 30\u001b[0m \u001b[43msd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sounddevice.py:398\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(ignore_errors)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for `play()`/`rec()`/`playrec()` to be finished.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03mPlayback/recording can be stopped with a `KeyboardInterrupt`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m \n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _last_callback:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_last_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sounddevice.py:2653\u001b[0m, in \u001b[0;36m_CallbackContext.wait\u001b[0;34m(self, ignore_errors)\u001b[0m\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for finished_callback.\u001b[39;00m\n\u001b[1;32m   2648\u001b[0m \n\u001b[1;32m   2649\u001b[0m \u001b[38;5;124;03mCan be interrupted with a KeyboardInterrupt.\u001b[39;00m\n\u001b[1;32m   2650\u001b[0m \n\u001b[1;32m   2651\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2653\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2655\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mclose(ignore_errors)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "import requests\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "youtube_url = \"https://www.youtube.com/watch?v=Szox9wD4HRU\"\n",
    "\n",
    "# Step 1: Extract direct audio URL\n",
    "ydl_opts = {'format': 'bestaudio/best'}\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    info = ydl.extract_info(youtube_url, download=False)\n",
    "    audio_url = info['url']\n",
    "\n",
    "# Step 2: Download audio into memory\n",
    "response = requests.get(audio_url, stream=True)\n",
    "audio_bytes = io.BytesIO(response.content)\n",
    "\n",
    "# Step 3: Decode audio (pydub will handle formats like webm or m4a)\n",
    "audio = AudioSegment.from_file(audio_bytes)\n",
    "\n",
    "# Convert to numpy array (samples)\n",
    "samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
    "samples /= np.iinfo(audio.array_type).max  # normalize to [-1, 1]\n",
    "\n",
    "# Step 4: Play audio with sounddevice\n",
    "print(\"Playing audio...\")\n",
    "sd.play(samples, samplerate=audio.frame_rate)\n",
    "sd.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec55419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Szox9wD4HRU\n",
      "[youtube] Szox9wD4HRU: Downloading webpage\n",
      "[youtube] Szox9wD4HRU: Downloading tv client config\n",
      "[youtube] Szox9wD4HRU: Downloading tv player API JSON\n",
      "[youtube] Szox9wD4HRU: Downloading ios player API JSON\n",
      "[youtube] Szox9wD4HRU: Downloading m3u8 information\n",
      "[info] Szox9wD4HRU: Downloading 1 format(s): 251\n",
      "[download] Destination: Data/downloaded_audio.webm\n",
      "[download] 100% of  968.34KiB in 00:00:00 at 1.93MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExtractAudio] Destination: Data/downloaded_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file Data/downloaded_audio.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.youtube.com/watch?v=Szox9wD4HRU\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    # 'outtmpl': 'Data/downloaded_audio.%(ext)s',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    # ydl.download([url])\n",
    "    info_dict = ydl.extract_info(youtube_url, download=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee731cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='../Data/audio.wav'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign file names\n",
    "input_file = \"../Data/downloaded_audio.mp3\"\n",
    "output_file = \"../Data/audio.wav\"\n",
    "\n",
    "# Convert mp3 file to wav file\n",
    "sound = AudioSegment.from_mp3(input_file)\n",
    "sound.export(output_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1640ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSpeechToText(file_name= \"audio.wav\"):\n",
    "    asr_model = EncoderDecoderASR.from_hparams(\n",
    "        source=\"speechbrain/asr-conformer-transformerlm-librispeech\", savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\", run_opts={\"device\": \"mps\"} )\n",
    "    text = asr_model.transcribe_file(file_name)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f7793e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Output channels > 65536 not supported at the MPS device. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transcript \u001b[38;5;241m=\u001b[39m \u001b[43mconvertSpeechToText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Data/downloaded_audio.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscript:\u001b[39m\u001b[38;5;124m\"\u001b[39m, transcript)\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mconvertSpeechToText\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvertSpeechToText\u001b[39m(file_name\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     asr_model \u001b[38;5;241m=\u001b[39m EncoderDecoderASR\u001b[38;5;241m.\u001b[39mfrom_hparams(\n\u001b[1;32m      3\u001b[0m         source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeechbrain/asr-conformer-transformerlm-librispeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, savedir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_models/asr-transformer-transformerlm-librispeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, run_opts\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m} )\n\u001b[0;32m----> 4\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43masr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/inference/ASR.py:89\u001b[0m, in \u001b[0;36mEncoderDecoderASR.transcribe_file\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranscribe_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transcribes the given audiofile into a sequence of words.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m        The audiofile transcription produced by this ASR system.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Fake a batch:\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     batch \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/inference/interfaces.py:305\u001b[0m, in \u001b[0;36mPretrained.load_audio\u001b[0;34m(self, path, savedir)\u001b[0m\n\u001b[1;32m    303\u001b[0m signal, sr \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mstr\u001b[39m(path), channels_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    304\u001b[0m signal \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_normalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/dataio/preprocess.py:69\u001b[0m, in \u001b[0;36mAudioNormalizer.__call__\u001b[0;34m(self, audio, sample_rate)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_resamplers[sample_rate] \u001b[38;5;241m=\u001b[39m Resample(\n\u001b[1;32m     66\u001b[0m         sample_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate\n\u001b[1;32m     67\u001b[0m     )\n\u001b[1;32m     68\u001b[0m resampler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_resamplers[sample_rate]\n\u001b[0;32m---> 69\u001b[0m resampled \u001b[38;5;241m=\u001b[39m \u001b[43mresampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mix(resampled)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/augment/time_domain.py:591\u001b[0m, in \u001b[0;36mResample.forward\u001b[0;34m(self, waveforms)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresampler\u001b[38;5;241m.\u001b[39mto(waveforms\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# in-place\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# Do resampling\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m resampled_waveform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unsqueezed:\n\u001b[1;32m    594\u001b[0m     resampled_waveform \u001b[38;5;241m=\u001b[39m resampled_waveform\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:979\u001b[0m, in \u001b[0;36mResample.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_freq:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m waveform\n\u001b[0;32m--> 979\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_sinc_resample_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchaudio/functional/functional.py:1466\u001b[0m, in \u001b[0;36m_apply_sinc_resample_kernel\u001b[0;34m(waveform, orig_freq, new_freq, gcd, kernel, width)\u001b[0m\n\u001b[1;32m   1464\u001b[0m num_wavs, length \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1465\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(waveform, (width, width \u001b[38;5;241m+\u001b[39m orig_freq))\n\u001b[0;32m-> 1466\u001b[0m resampled \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m resampled \u001b[38;5;241m=\u001b[39m resampled\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(num_wavs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1468\u001b[0m target_length \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mceil(torch\u001b[38;5;241m.\u001b[39mas_tensor(new_freq \u001b[38;5;241m*\u001b[39m length \u001b[38;5;241m/\u001b[39m orig_freq))\u001b[38;5;241m.\u001b[39mlong()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Output channels > 65536 not supported at the MPS device. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "transcript = convertSpeechToText(file_name=\"../Data/downloaded_audio.mp3\")\n",
    "print(\"Transcript:\", transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78211511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import requests\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "from speechbrain.pretrained import EncoderDecoderASR\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def convertSpeechToText_from_youtube(youtube_url: str):\n",
    "    # Step 1: Extract best audio URL (metadata only, no file download)\n",
    "    ydl_opts = {'format': 'bestaudio/best'}\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(youtube_url, download=False)\n",
    "        audio_url = info['url']\n",
    "\n",
    "    # Step 2: Stream audio bytes into memory\n",
    "    response = requests.get(audio_url, stream=True)\n",
    "    audio_bytes = io.BytesIO(response.content)\n",
    "\n",
    "    # Step 3: Convert to WAV using pydub (kept in memory)\n",
    "    audio = AudioSegment.from_file(audio_bytes)\n",
    "    \n",
    "    # Save temporarily as wav in a temp file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmpfile:\n",
    "        audio.export(tmpfile.name, format=\"wav\")\n",
    "        temp_wav_path = tmpfile.name\n",
    "\n",
    "    # Step 4: Speech-to-text using SpeechBrain\n",
    "    asr_model = EncoderDecoderASR.from_hparams(\n",
    "        source=\"speechbrain/asr-conformer-transformerlm-librispeech\",\n",
    "        savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\"\n",
    "    )\n",
    "\n",
    "    text = asr_model.transcribe_file(temp_wav_path)\n",
    "\n",
    "    # Clean up temp file\n",
    "    os.remove(temp_wav_path)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "youtube_url = \"https://www.youtube.com/watch?v=YOUR_VIDEO_ID\"\n",
    "transcription = convertSpeechToText_from_youtube(youtube_url)\n",
    "print(\"Transcription:\")\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddac87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Provide your YouTube video link\n",
    "# video_url = \"https://www.youtube.com/watch?v=Szox9wD4HRU\"\n",
    "\n",
    "# # Create YouTube object\n",
    "# yt = YouTube(video_url)\n",
    "\n",
    "# # Filter and get the audio-only stream (usually .webm or .mp4 audio)\n",
    "# audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "\n",
    "# # Download audio\n",
    "# audio_stream.download(output_path=\"./Data/\", filename=\"audio.mp3\")  # mp3 extension can be set manually\n",
    "# print(\"Audio downloaded as audio.mp3\")\n",
    "\n",
    "# from pydub import AudioSegment\n",
    "\n",
    "# # Convert audio.webm or audio.mp4 to audio.mp3\n",
    "# input_audio = \"audio.webm\"  # change based on actual filename\n",
    "# output_audio = \"converted_audio.mp3\"\n",
    "\n",
    "# # Convert\n",
    "# sound = AudioSegment.from_file(input_audio)\n",
    "# sound.export(output_audio, format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85edc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ambarishg/open-source-llms/tree/main/llama2/\n",
    "# from ctransformers import AutoModelForCausalLM as AML\n",
    "# from pprint import pprint\n",
    "\n",
    "# llm = AML.from_pretrained('llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
    "#                                             model_type='llama')\n",
    "\n",
    "# pprint(llm('AI is going to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd4a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"HOW TO TALK ABOUT DAILY ROUTINES MY DAILY ROUTINE IS NOTHING SPECIAL I USUALLY WAKE UP EARLY AROUND SEVEN A M BREAKFAST IS NOT IMPORTANT TO ME SO I USUALLY ONLY DRINK A CUP OF COFFEE SOMETIMES THOUGH I WILL HAVE AN APPLE BREAKFAST IS FOLLOWED BY A SHOWER AFTER I GET DRESSED I GO TO WORK WORK IS ALWAYS THE SAME DURING THE MORNINGS I RESPOND TO THE MEALS AND SET UP MEETINGS AT LUNCH TIME I START TO FEEL REALLY HUNGRY OFTEN MY FRIENDS AND I MEET AT A CAFE TO HAVE LUNCH TOGETHER WHEN DINNER TIME COMES AROUND HOWEVER I COOKED DINNER AT NIGHT AFTER DINNER I TAKE MY DOG FOR A WALK THROUGH THE PARK WE CAN USUALLY SEE THE SUNSET WHICH IS QUITE BEAUTIFUL I WATCHED TEEVIE'S SERIES OR MOVIES AT NIGHT THERE IS NOTHING LIKE A GOOD MOVIE BEFORE BED I SLEEP AT TEN P M AND THEN START THE WHOLE ROUTINE OVER AGAIN THE NEXT DAY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a341788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaContextQA:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            use_auth_token=True\n",
    "        )\n",
    "        self.context = None\n",
    "        self.history = []  # Stores list of {\"question\": ..., \"answer\": ..., \"timestamp\": ...}\n",
    "\n",
    "    def set_context(self, context, append=False):\n",
    "        \"\"\"Set or append to context.\"\"\"\n",
    "        if append and self.context:\n",
    "            self.context += \"\\n\" + context.strip()\n",
    "        else:\n",
    "            self.context = context.strip()\n",
    "\n",
    "    def clear_context(self):\n",
    "        \"\"\"Remove current context and history.\"\"\"\n",
    "        self.context = None\n",
    "        self.history = []\n",
    "\n",
    "    def ask_question(self, question, max_new_tokens=150):\n",
    "        \"\"\"Ask a question using current context and store history.\"\"\"\n",
    "        if not self.context:\n",
    "            return \"No context has been set.\"\n",
    "\n",
    "        prompt = f\"\"\"[INST] <<SYS>>\n",
    "            You are a helpful assistant.\n",
    "            <</SYS>>\n",
    "            Given the following content, answer the question concisely.\n",
    "\n",
    "            Content:\n",
    "            {self.context}\n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "            [/INST]\n",
    "            \"\"\"\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Clean up output\n",
    "        answer_start = response.find(\"Question:\")\n",
    "        if answer_start != -1:\n",
    "            answer = response[answer_start + len(\"Question:\"):].strip()\n",
    "        else:\n",
    "            answer = response.strip()\n",
    "\n",
    "        # Save to history\n",
    "        self.history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def save_session(self, filepath=\"qa_session.json\"):\n",
    "        \"\"\"Save context and Q&A history to a JSON file.\"\"\"\n",
    "        data = {\n",
    "            \"context\": self.context,\n",
    "            \"history\": self.history\n",
    "        }\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def load_session(self, filepath=\"qa_session.json\"):\n",
    "        \"\"\"Load context and Q&A history from a JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            self.context = data.get(\"context\", \"\")\n",
    "            self.history = data.get(\"history\", [])\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load session: {e}\")\n",
    "\n",
    "    def print_history(self):\n",
    "        \"\"\"Print Q&A history in a readable format.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No question-answer history found.\")\n",
    "            return\n",
    "\n",
    "        print(\"Conversation History:\")\n",
    "        for i, entry in enumerate(self.history, 1):\n",
    "            print(f\"\\n Question {i} [{entry['timestamp']}]:\")\n",
    "            print(f\"   Question: {entry['question']}\")\n",
    "            print(f\"   Answer: {entry['answer']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b2e580",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'meta-llama/Llama-2-7b-chat-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-chat-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaContextQA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set context\u001b[39;00m\n\u001b[1;32m      4\u001b[0m qa\u001b[38;5;241m.\u001b[39mset_context(transcript)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mLlamaContextQA.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m         model_name,\n\u001b[1;32m      6\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m      7\u001b[0m         device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1027\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2051\u001b[0m     )\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'meta-llama/Llama-2-7b-chat-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-chat-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "qa = LlamaContextQA(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Set context\n",
    "qa.set_context(transcript)\n",
    "\n",
    "\n",
    "# def ask(question):\n",
    "#     user_prompt = f\"[INST] {system_prompt}\\n{question} [/INST]\"\n",
    "#     inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "\n",
    "while True:\n",
    "    question = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "    if question.lower() == 'exit':\n",
    "        break\n",
    "    answer = qa.ask_question(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "\n",
    "# Save session\n",
    "# qa.save_session(\"Data/session.json\")\n",
    "\n",
    "# Clear and reload\n",
    "# qa.clear_context()\n",
    "# qa.load_session(\"session.json\")\n",
    "\n",
    "# Check reloaded history\n",
    "# for entry in qa.history:\n",
    "#     print(f\"[{entry['timestamp']}] Q: {entry['question']} ‚Üí A: {entry['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d79bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY Q&A HISTORY\n",
    "def display_history(qa, height=400):\n",
    "    \"\"\"Display Q&A history in the sidebar in a scrollable, expandable format.\"\"\"\n",
    "    with st.sidebar:\n",
    "        st.subheader(\"üìú Q&A History\")\n",
    "\n",
    "        if not qa.history:\n",
    "            st.info(\"üì≠ No history yet.\")\n",
    "            return\n",
    "\n",
    "        st.markdown(f\"<div style='height: {height}px; overflow-y: auto;'>\", unsafe_allow_html=True)\n",
    "\n",
    "        for i, entry in enumerate(qa.history, 1):\n",
    "            with st.expander(f\"üîπ Q{i} ‚Äî {entry['timestamp']}\"):\n",
    "                st.markdown(f\"**Q:** {entry['question']}\")\n",
    "                st.markdown(f\"**A:** {entry['answer']}\")\n",
    "\n",
    "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# STREAMLIT UI\n",
    "st.set_page_config(page_title=\"ü¶ô LLaMA Contextual Q&A\", layout=\"centered\")\n",
    "st.title(\"ü¶ô LLaMA Contextual Q&A Assistant\")\n",
    "\n",
    "# Initialize session state\n",
    "if \"qa\" not in st.session_state:\n",
    "    st.session_state.qa = LlamaContextQA(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "qa = st.session_state.qa\n",
    "\n",
    "# Sidebar controls\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Controls\")\n",
    "\n",
    "    # Save session with user-defined filename\n",
    "    save_filename = st.text_input(\"Save As:\", value=\"qa_session.json\")\n",
    "    if st.button(\"üíæ Save Session\"):\n",
    "        if save_filename:\n",
    "            try:\n",
    "                qa.save_session(save_filename)\n",
    "                st.success(f\"Session saved to `{save_filename}`.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Failed to save: {e}\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a filename.\")\n",
    "\n",
    "    # Load session from file uploader\n",
    "    uploaded_file = st.file_uploader(\"Load Session (.json)\", type=\"json\")\n",
    "    if uploaded_file is not None:\n",
    "        try:\n",
    "            qa.load_session(uploaded_file)\n",
    "            st.success(\"Session loaded from uploaded file.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to load: {e}\")\n",
    "\n",
    "    # Clear session\n",
    "    if st.button(\"Clear Context & History\"):\n",
    "        qa.clear_context()\n",
    "        st.warning(\"Context and history cleared.\")\n",
    "\n",
    "# Set or update context\n",
    "with st.expander(\"Set or Update Context\"):\n",
    "    new_context = st.text_area(\"Enter context (you can update this at any time):\", height=150)\n",
    "    append = st.checkbox(\"Append to existing context\", value=False)\n",
    "    if st.button(\"Apply Context\"):\n",
    "        if new_context.strip():\n",
    "            qa.set_context(new_context, append=append)\n",
    "            st.success(\"üß† Context updated.\")\n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è Please enter some context.\")\n",
    "\n",
    "# Ask a question\n",
    "st.subheader(\"Ask a Question\")\n",
    "question = st.text_input(\"Type your question here:\")\n",
    "if st.button(\"Get Answer\"):\n",
    "    if not question.strip():\n",
    "        st.warning(\"Please enter a question.\")\n",
    "    else:\n",
    "        with st.spinner(\"üí¨ Thinking... generating answer...\"):\n",
    "            answer = qa.ask_question(question)\n",
    "            qa.save_session()  # Auto-save after each question\n",
    "        st.success(\"Answer:\")\n",
    "        st.write(answer)\n",
    "\n",
    "# Always show scrollable history in sidebar\n",
    "display_history(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c55eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85eb293b16f4e1799ed1ff69e7f5ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b94d5f1c7654ba4ae448b0c9ff69dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11535d010ab8414483c03f7a4f22b95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5033f8e245c4da29f228c72ca272c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Choose your LLaMA model (must be downloaded from Hugging Face)    \n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # or another variant\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=True,\n",
    ")\n",
    "\n",
    "# Sample content and question\n",
    "context = \"\"\"\n",
    "Deep learning has revolutionized natural language processing by enabling models like transformers to learn contextual relationships in text. These models, including BERT and GPT, are pre-trained on large corpora and can be fine-tuned for various downstream tasks.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which models are mentioned as examples of deep learning in NLP?\"\n",
    "\n",
    "# Format prompt\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful assistant.\n",
    "<</SYS>>\n",
    "Given the following content, answer the question concisely.\n",
    "\n",
    "Content:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Clean up response\n",
    "answer_start = response.find(\"Question:\")\n",
    "answer = response[answer_start + len(\"Question:\"):].strip()\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5a09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# # Define context (system message)\n",
    "# context = \"\"\"Deep learning has revolutionized natural language processing by enabling models like transformers to learn contextual relationships in text. These models, including BERT and GPT, are pre-trained on large corpora and can be fine-tuned for various downstream tasks.\"\"\"\n",
    "\n",
    "# # Format a reusable system prompt\n",
    "# system_prompt = f\"<<SYS>>\\nThe following is a context:\\n{context}\\n<</SYS>>\\n\"\n",
    "\n",
    "# # Track previous interaction (optional)\n",
    "# chat_history = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt}\n",
    "# ]\n",
    "\n",
    "# def ask(question):\n",
    "#     user_prompt = f\"[INST] {system_prompt}\\n{question} [/INST]\"\n",
    "#     inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "# # Example usage:\n",
    "# print(ask(\"Which models are mentioned?\"))\n",
    "# print(ask(\"What are those models trained on?\"))\n",
    "# print(ask(\"What task can they be fine-tuned for?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd11aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
