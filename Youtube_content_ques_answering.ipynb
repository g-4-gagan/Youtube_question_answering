{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9954b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61af1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from pytube import YouTube\n",
    "from speechbrain.inference import EncoderDecoderASR\n",
    "import streamlit as st\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec55419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Szox9wD4HRU\n",
      "[youtube] Szox9wD4HRU: Downloading webpage\n",
      "[youtube] Szox9wD4HRU: Downloading tv client config\n",
      "[youtube] Szox9wD4HRU: Downloading tv player API JSON\n",
      "[youtube] Szox9wD4HRU: Downloading ios player API JSON\n",
      "[youtube] Szox9wD4HRU: Downloading m3u8 information\n",
      "[info] Szox9wD4HRU: Downloading 1 format(s): 251\n",
      "[download] Destination: Data/downloaded_audio.webm\n",
      "[download] 100% of  968.34KiB in 00:00:00 at 1.93MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExtractAudio] Destination: Data/downloaded_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file Data/downloaded_audio.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.youtube.com/watch?v=Szox9wD4HRU\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': 'Data/downloaded_audio.%(ext)s',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee731cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='../Data/audio.wav'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign file names\n",
    "input_file = \"../Data/downloaded_audio.mp3\"\n",
    "output_file = \"../Data/audio.wav\"\n",
    "\n",
    "# Convert mp3 file to wav file\n",
    "sound = AudioSegment.from_mp3(input_file)\n",
    "sound.export(output_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc1640ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSpeechToText(file_name= \"audio.wav\"):\n",
    "    asr_model = EncoderDecoderASR.from_hparams(\n",
    "        source=\"speechbrain/asr-conformer-transformerlm-librispeech\", savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\")\n",
    "    text = asr_model.transcribe_file(file_name)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7793e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transcript \u001b[38;5;241m=\u001b[39m \u001b[43mconvertSpeechToText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Data/audio.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscript:\u001b[39m\u001b[38;5;124m\"\u001b[39m, transcript)\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mconvertSpeechToText\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvertSpeechToText\u001b[39m(file_name\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     asr_model \u001b[38;5;241m=\u001b[39m EncoderDecoderASR\u001b[38;5;241m.\u001b[39mfrom_hparams(\n\u001b[1;32m      3\u001b[0m         source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeechbrain/asr-conformer-transformerlm-librispeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, savedir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained_models/asr-transformer-transformerlm-librispeech\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43masr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/inference/ASR.py:93\u001b[0m, in \u001b[0;36mEncoderDecoderASR.transcribe_file\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m batch \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     92\u001b[0m rel_length \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m])\n\u001b[0;32m---> 93\u001b[0m predicted_words, predicted_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_length\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_words[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/inference/ASR.py:162\u001b[0m, in \u001b[0;36mEncoderDecoderASR.transcribe_batch\u001b[0;34m(self, wavs, wav_lens)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m [encoder_out, wav_lens]\n\u001b[0;32m--> 162\u001b[0m     predicted_tokens, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     predicted_words \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode_ids(token_seq)\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token_seq \u001b[38;5;129;01min\u001b[39;00m predicted_tokens\n\u001b[1;32m    166\u001b[0m     ]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_words, predicted_tokens\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/decoders/seq2seq.py:1608\u001b[0m, in \u001b[0;36mS2SBeamSearcher.forward\u001b[0;34m(self, enc_states, wav_len)\u001b[0m\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_full_beams(eos_hyps_and_log_probs_scores):\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1598\u001b[0m (\n\u001b[1;32m   1599\u001b[0m     alived_hyps,\n\u001b[1;32m   1600\u001b[0m     inp_tokens,\n\u001b[1;32m   1601\u001b[0m     log_probs,\n\u001b[1;32m   1602\u001b[0m     eos_hyps_and_log_probs_scores,\n\u001b[1;32m   1603\u001b[0m     memory,\n\u001b[1;32m   1604\u001b[0m     scorer_memory,\n\u001b[1;32m   1605\u001b[0m     attn,\n\u001b[1;32m   1606\u001b[0m     prev_attn_peak,\n\u001b[1;32m   1607\u001b[0m     scores,\n\u001b[0;32m-> 1608\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43malived_hyps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43minp_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_hyps_and_log_probs_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscorer_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_attn_peak\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43menc_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43menc_lens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_end_condition(alived_hyps):\n\u001b[1;32m   1623\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/decoders/seq2seq.py:1486\u001b[0m, in \u001b[0;36mS2SBeamSearcher.search_step\u001b[0;34m(self, alived_hyps, inp_tokens, log_probs, eos_hyps_and_log_probs_scores, memory, scorer_memory, attn, prev_attn_peak, enc_states, enc_lens, step)\u001b[0m\n\u001b[1;32m   1480\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_eos_minus_inf_step(\n\u001b[1;32m   1481\u001b[0m     log_probs, step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_decode_steps\n\u001b[1;32m   1482\u001b[0m )\n\u001b[1;32m   1484\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eos_threshold_step(log_probs)\n\u001b[0;32m-> 1486\u001b[0m (log_probs, scorer_memory) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scorer_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43minp_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1490\u001b[0m (\n\u001b[1;32m   1491\u001b[0m     scores,\n\u001b[1;32m   1492\u001b[0m     candidates,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1497\u001b[0m     alived_hyps, log_probs, step\n\u001b[1;32m   1498\u001b[0m )\n\u001b[1;32m   1500\u001b[0m memory, scorer_memory, prev_attn_peak \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_permute_memory(\n\u001b[1;32m   1501\u001b[0m     memory, scorer_memory, predecessors, candidates, prev_attn_peak\n\u001b[1;32m   1502\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/decoders/seq2seq.py:900\u001b[0m, in \u001b[0;36mS2SBeamSearcher._scorer_step\u001b[0;34m(self, inp_tokens, scorer_memory, attn, log_probs)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This method call the scorers if scorer is not None.\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \n\u001b[1;32m    880\u001b[0m \u001b[38;5;124;03mArguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    The memory variables generated in this step.\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscorer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     log_probs, scorer_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43minp_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_size\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_probs, scorer_memory\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/decoders/scorer.py:1202\u001b[0m, in \u001b[0;36mScorerBuilder.score\u001b[0;34m(self, inp_tokens, memory, attn, log_probs, beam_size)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1199\u001b[0m         \u001b[38;5;66;03m# block blank token if CTC is used\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m         log_probs[:, impl\u001b[38;5;241m.\u001b[39mblank_index] \u001b[38;5;241m=\u001b[39m impl\u001b[38;5;241m.\u001b[39mctc_score\u001b[38;5;241m.\u001b[39mminus_inf\n\u001b[0;32m-> 1202\u001b[0m     score, new_memory[k] \u001b[38;5;241m=\u001b[39m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m     log_probs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m score \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[k]\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;66;03m# select candidates from the results of full scorers for partial scorers\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/decoders/scorer.py:205\u001b[0m, in \u001b[0;36mCTCScorer.score\u001b[0;34m(self, inp_tokens, memory, candidates, attn)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp_tokens, memory, candidates, attn):\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This method scores the new beams based on the\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    CTC scores computed over the time frames.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    memory\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     scores, memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43minp_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores, memory\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/speechbrain/decoders/ctc.py:169\u001b[0m, in \u001b[0;36mCTCPrefixScore.forward_step\u001b[0;34m(self, inp_tokens, states, candidates, attn)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Prepare forward probs\u001b[39;00m\n\u001b[1;32m    164\u001b[0m r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[1;32m    165\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_enc_len, \u001b[38;5;241m2\u001b[39m, n_bh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_candidates),\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminus_inf,\n\u001b[1;32m    167\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    168\u001b[0m )\n\u001b[0;32m--> 169\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminus_inf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# (Alg.2-6)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transcript = convertSpeechToText(file_name=\"../Data/downloaded_audio.mp3\")\n",
    "print(\"Transcript:\", transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddac87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Provide your YouTube video link\n",
    "# video_url = \"https://www.youtube.com/watch?v=Szox9wD4HRU\"\n",
    "\n",
    "# # Create YouTube object\n",
    "# yt = YouTube(video_url)\n",
    "\n",
    "# # Filter and get the audio-only stream (usually .webm or .mp4 audio)\n",
    "# audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "\n",
    "# # Download audio\n",
    "# audio_stream.download(output_path=\"./Data/\", filename=\"audio.mp3\")  # mp3 extension can be set manually\n",
    "# print(\"Audio downloaded as audio.mp3\")\n",
    "\n",
    "# from pydub import AudioSegment\n",
    "\n",
    "# # Convert audio.webm or audio.mp4 to audio.mp3\n",
    "# input_audio = \"audio.webm\"  # change based on actual filename\n",
    "# output_audio = \"converted_audio.mp3\"\n",
    "\n",
    "# # Convert\n",
    "# sound = AudioSegment.from_file(input_audio)\n",
    "# sound.export(output_audio, format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85edc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ambarishg/open-source-llms/tree/main/llama2/\n",
    "# from ctransformers import AutoModelForCausalLM as AML\n",
    "# from pprint import pprint\n",
    "\n",
    "# llm = AML.from_pretrained('llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
    "#                                             model_type='llama')\n",
    "\n",
    "# pprint(llm('AI is going to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd4a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"HOW TO TALK ABOUT DAILY ROUTINES MY DAILY ROUTINE IS NOTHING SPECIAL I USUALLY WAKE UP EARLY AROUND SEVEN A M BREAKFAST IS NOT IMPORTANT TO ME SO I USUALLY ONLY DRINK A CUP OF COFFEE SOMETIMES THOUGH I WILL HAVE AN APPLE BREAKFAST IS FOLLOWED BY A SHOWER AFTER I GET DRESSED I GO TO WORK WORK IS ALWAYS THE SAME DURING THE MORNINGS I RESPOND TO THE MEALS AND SET UP MEETINGS AT LUNCH TIME I START TO FEEL REALLY HUNGRY OFTEN MY FRIENDS AND I MEET AT A CAFE TO HAVE LUNCH TOGETHER WHEN DINNER TIME COMES AROUND HOWEVER I COOKED DINNER AT NIGHT AFTER DINNER I TAKE MY DOG FOR A WALK THROUGH THE PARK WE CAN USUALLY SEE THE SUNSET WHICH IS QUITE BEAUTIFUL I WATCHED TEEVIE'S SERIES OR MOVIES AT NIGHT THERE IS NOTHING LIKE A GOOD MOVIE BEFORE BED I SLEEP AT TEN P M AND THEN START THE WHOLE ROUTINE OVER AGAIN THE NEXT DAY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a341788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaContextQA:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            use_auth_token=True\n",
    "        )\n",
    "        self.context = None\n",
    "        self.history = []  # Stores list of {\"question\": ..., \"answer\": ..., \"timestamp\": ...}\n",
    "\n",
    "    def set_context(self, context, append=False):\n",
    "        \"\"\"Set or append to context.\"\"\"\n",
    "        if append and self.context:\n",
    "            self.context += \"\\n\" + context.strip()\n",
    "        else:\n",
    "            self.context = context.strip()\n",
    "\n",
    "    def clear_context(self):\n",
    "        \"\"\"Remove current context and history.\"\"\"\n",
    "        self.context = None\n",
    "        self.history = []\n",
    "\n",
    "    def ask_question(self, question, max_new_tokens=150):\n",
    "        \"\"\"Ask a question using current context and store history.\"\"\"\n",
    "        if not self.context:\n",
    "            return \"No context has been set.\"\n",
    "\n",
    "        prompt = f\"\"\"[INST] <<SYS>>\n",
    "            You are a helpful assistant.\n",
    "            <</SYS>>\n",
    "            Given the following content, answer the question concisely.\n",
    "\n",
    "            Content:\n",
    "            {self.context}\n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "            [/INST]\n",
    "            \"\"\"\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Clean up output\n",
    "        answer_start = response.find(\"Question:\")\n",
    "        if answer_start != -1:\n",
    "            answer = response[answer_start + len(\"Question:\"):].strip()\n",
    "        else:\n",
    "            answer = response.strip()\n",
    "\n",
    "        # Save to history\n",
    "        self.history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def save_session(self, filepath=\"qa_session.json\"):\n",
    "        \"\"\"Save context and Q&A history to a JSON file.\"\"\"\n",
    "        data = {\n",
    "            \"context\": self.context,\n",
    "            \"history\": self.history\n",
    "        }\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def load_session(self, filepath=\"qa_session.json\"):\n",
    "        \"\"\"Load context and Q&A history from a JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            self.context = data.get(\"context\", \"\")\n",
    "            self.history = data.get(\"history\", [])\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load session: {e}\")\n",
    "\n",
    "    def print_history(self):\n",
    "        \"\"\"Print Q&A history in a readable format.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No question-answer history found.\")\n",
    "            return\n",
    "\n",
    "        print(\"Conversation History:\")\n",
    "        for i, entry in enumerate(self.history, 1):\n",
    "            print(f\"\\n Question {i} [{entry['timestamp']}]:\")\n",
    "            print(f\"   Question: {entry['question']}\")\n",
    "            print(f\"   Answer: {entry['answer']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b2e580",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'meta-llama/Llama-2-7b-chat-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-chat-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaContextQA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set context\u001b[39;00m\n\u001b[1;32m      4\u001b[0m qa\u001b[38;5;241m.\u001b[39mset_context(transcript)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mLlamaContextQA.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      5\u001b[0m         model_name,\n\u001b[1;32m      6\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m      7\u001b[0m         device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1027\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1024\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2044\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2050\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2051\u001b[0m     )\n\u001b[1;32m   2053\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'meta-llama/Llama-2-7b-chat-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-chat-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "qa = LlamaContextQA(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Set context\n",
    "qa.set_context(transcript)\n",
    "\n",
    "\n",
    "# def ask(question):\n",
    "#     user_prompt = f\"[INST] {system_prompt}\\n{question} [/INST]\"\n",
    "#     inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "\n",
    "while True:\n",
    "    question = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "    if question.lower() == 'exit':\n",
    "        break\n",
    "    answer = qa.ask_question(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "\n",
    "# Save session\n",
    "# qa.save_session(\"Data/session.json\")\n",
    "\n",
    "# Clear and reload\n",
    "# qa.clear_context()\n",
    "# qa.load_session(\"session.json\")\n",
    "\n",
    "# Check reloaded history\n",
    "# for entry in qa.history:\n",
    "#     print(f\"[{entry['timestamp']}] Q: {entry['question']} ‚Üí A: {entry['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d79bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY Q&A HISTORY\n",
    "def display_history(qa, height=400):\n",
    "    \"\"\"Display Q&A history in the sidebar in a scrollable, expandable format.\"\"\"\n",
    "    with st.sidebar:\n",
    "        st.subheader(\"üìú Q&A History\")\n",
    "\n",
    "        if not qa.history:\n",
    "            st.info(\"üì≠ No history yet.\")\n",
    "            return\n",
    "\n",
    "        st.markdown(f\"<div style='height: {height}px; overflow-y: auto;'>\", unsafe_allow_html=True)\n",
    "\n",
    "        for i, entry in enumerate(qa.history, 1):\n",
    "            with st.expander(f\"üîπ Q{i} ‚Äî {entry['timestamp']}\"):\n",
    "                st.markdown(f\"**Q:** {entry['question']}\")\n",
    "                st.markdown(f\"**A:** {entry['answer']}\")\n",
    "\n",
    "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# STREAMLIT UI\n",
    "st.set_page_config(page_title=\"ü¶ô LLaMA Contextual Q&A\", layout=\"centered\")\n",
    "st.title(\"ü¶ô LLaMA Contextual Q&A Assistant\")\n",
    "\n",
    "# Initialize session state\n",
    "if \"qa\" not in st.session_state:\n",
    "    st.session_state.qa = LlamaContextQA(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "qa = st.session_state.qa\n",
    "\n",
    "# Sidebar controls\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Controls\")\n",
    "\n",
    "    # Save session with user-defined filename\n",
    "    save_filename = st.text_input(\"Save As:\", value=\"qa_session.json\")\n",
    "    if st.button(\"üíæ Save Session\"):\n",
    "        if save_filename:\n",
    "            try:\n",
    "                qa.save_session(save_filename)\n",
    "                st.success(f\"Session saved to `{save_filename}`.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Failed to save: {e}\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a filename.\")\n",
    "\n",
    "    # Load session from file uploader\n",
    "    uploaded_file = st.file_uploader(\"Load Session (.json)\", type=\"json\")\n",
    "    if uploaded_file is not None:\n",
    "        try:\n",
    "            qa.load_session(uploaded_file)\n",
    "            st.success(\"Session loaded from uploaded file.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to load: {e}\")\n",
    "\n",
    "    # Clear session\n",
    "    if st.button(\"Clear Context & History\"):\n",
    "        qa.clear_context()\n",
    "        st.warning(\"Context and history cleared.\")\n",
    "\n",
    "# Set or update context\n",
    "with st.expander(\"Set or Update Context\"):\n",
    "    new_context = st.text_area(\"Enter context (you can update this at any time):\", height=150)\n",
    "    append = st.checkbox(\"Append to existing context\", value=False)\n",
    "    if st.button(\"Apply Context\"):\n",
    "        if new_context.strip():\n",
    "            qa.set_context(new_context, append=append)\n",
    "            st.success(\"üß† Context updated.\")\n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è Please enter some context.\")\n",
    "\n",
    "# Ask a question\n",
    "st.subheader(\"Ask a Question\")\n",
    "question = st.text_input(\"Type your question here:\")\n",
    "if st.button(\"Get Answer\"):\n",
    "    if not question.strip():\n",
    "        st.warning(\"Please enter a question.\")\n",
    "    else:\n",
    "        with st.spinner(\"üí¨ Thinking... generating answer...\"):\n",
    "            answer = qa.ask_question(question)\n",
    "            qa.save_session()  # Auto-save after each question\n",
    "        st.success(\"Answer:\")\n",
    "        st.write(answer)\n",
    "\n",
    "# Always show scrollable history in sidebar\n",
    "display_history(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c55eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85eb293b16f4e1799ed1ff69e7f5ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b94d5f1c7654ba4ae448b0c9ff69dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11535d010ab8414483c03f7a4f22b95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5033f8e245c4da29f228c72ca272c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Choose your LLaMA model (must be downloaded from Hugging Face)    \n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # or another variant\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=True,\n",
    ")\n",
    "\n",
    "# Sample content and question\n",
    "context = \"\"\"\n",
    "Deep learning has revolutionized natural language processing by enabling models like transformers to learn contextual relationships in text. These models, including BERT and GPT, are pre-trained on large corpora and can be fine-tuned for various downstream tasks.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which models are mentioned as examples of deep learning in NLP?\"\n",
    "\n",
    "# Format prompt\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful assistant.\n",
    "<</SYS>>\n",
    "Given the following content, answer the question concisely.\n",
    "\n",
    "Content:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Clean up response\n",
    "answer_start = response.find(\"Question:\")\n",
    "answer = response[answer_start + len(\"Question:\"):].strip()\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5a09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# # Define context (system message)\n",
    "# context = \"\"\"Deep learning has revolutionized natural language processing by enabling models like transformers to learn contextual relationships in text. These models, including BERT and GPT, are pre-trained on large corpora and can be fine-tuned for various downstream tasks.\"\"\"\n",
    "\n",
    "# # Format a reusable system prompt\n",
    "# system_prompt = f\"<<SYS>>\\nThe following is a context:\\n{context}\\n<</SYS>>\\n\"\n",
    "\n",
    "# # Track previous interaction (optional)\n",
    "# chat_history = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt}\n",
    "# ]\n",
    "\n",
    "# def ask(question):\n",
    "#     user_prompt = f\"[INST] {system_prompt}\\n{question} [/INST]\"\n",
    "#     inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "# # Example usage:\n",
    "# print(ask(\"Which models are mentioned?\"))\n",
    "# print(ask(\"What are those models trained on?\"))\n",
    "# print(ask(\"What task can they be fine-tuned for?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd11aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
