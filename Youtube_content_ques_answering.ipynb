{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9954b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c61af1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "from pytube import YouTube\n",
    "from speechbrain.inference import EncoderDecoderASR\n",
    "import streamlit as st\n",
    "\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec55419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=Szox9wD4HRU\n",
      "[youtube] Szox9wD4HRU: Downloading webpage\n",
      "[youtube] Szox9wD4HRU: Downloading tv client config\n",
      "[youtube] Szox9wD4HRU: Downloading tv player API JSON\n",
      "[youtube] Szox9wD4HRU: Downloading ios player API JSON\n",
      "[youtube] Szox9wD4HRU: Downloading m3u8 information\n",
      "[info] Szox9wD4HRU: Downloading 1 format(s): 251\n",
      "[download] Destination: Data/downloaded_audio.webm\n",
      "[download] 100% of  968.34KiB in 00:00:00 at 1.93MiB/s   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ExtractAudio] Destination: Data/downloaded_audio.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting original file Data/downloaded_audio.webm (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.youtube.com/watch?v=Szox9wD4HRU\"\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': 'Data/downloaded_audio.%(ext)s',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([url])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1640ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSpeechToText(file_name= \"audio.wav\"):\n",
    "    asr_model = EncoderDecoderASR.from_hparams(\n",
    "        source=\"speechbrain/asr-conformer-transformerlm-librispeech\", savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\")\n",
    "    text = asr_model.transcribe_file(file_name)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f7793e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: HOW TO TALK ABOUT DAILY ROUTINES MY DAILY ROUTINE IS NOTHING SPECIAL I USUALLY WAKE UP EARLY AROUND SEVEN A M BREAKFAST IS NOT IMPORTANT TO ME SO I USUALLY ONLY DRINK A CUP OF COFFEE SOMETIMES THOUGH I WILL HAVE AN APPLE BREAKFAST IS FOLLOWED BY A SHOWER AFTER I GET DRESSED I GO TO WORK WORK IS ALWAYS THE SAME DURING THE MORNINGS I RESPOND TO THE MEALS AND SET UP MEETINGS AT LUNCH TIME I START TO FEEL REALLY HUNGRY OFTEN MY FRIENDS AND I MEET AT A CAFE TO HAVE LUNCH TOGETHER WHEN DINNER TIME COMES AROUND HOWEVER I COOKED DINNER AT NIGHT AFTER DINNER I TAKE MY DOG FOR A WALK THROUGH THE PARK WE CAN USUALLY SEE THE SUNSET WHICH IS QUITE BEAUTIFUL I WATCHED TEEVIE'S SERIES OR MOVIES AT NIGHT THERE IS NOTHING LIKE A GOOD MOVIE BEFORE BED I SLEEP AT TEN P M AND THEN START THE WHOLE ROUTINE OVER AGAIN THE NEXT DAY\n"
     ]
    }
   ],
   "source": [
    "transcript = convertSpeechToText(file_name=\"Data/downloaded_audio.mp3\")\n",
    "print(\"Transcript:\", transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fddac87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Provide your YouTube video link\n",
    "# video_url = \"https://www.youtube.com/watch?v=Szox9wD4HRU\"\n",
    "\n",
    "# # Create YouTube object\n",
    "# yt = YouTube(video_url)\n",
    "\n",
    "# # Filter and get the audio-only stream (usually .webm or .mp4 audio)\n",
    "# audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "\n",
    "# # Download audio\n",
    "# audio_stream.download(output_path=\"./Data/\", filename=\"audio.mp3\")  # mp3 extension can be set manually\n",
    "# print(\"Audio downloaded as audio.mp3\")\n",
    "\n",
    "# from pydub import AudioSegment\n",
    "\n",
    "# # Convert audio.webm or audio.mp4 to audio.mp3\n",
    "# input_audio = \"audio.webm\"  # change based on actual filename\n",
    "# output_audio = \"converted_audio.mp3\"\n",
    "\n",
    "# # Convert\n",
    "# sound = AudioSegment.from_file(input_audio)\n",
    "# sound.export(output_audio, format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85edc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_0.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ambarishg/open-source-llms/tree/main/llama2/\n",
    "# from ctransformers import AutoModelForCausalLM as AML\n",
    "# from pprint import pprint\n",
    "\n",
    "# llm = AML.from_pretrained('llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
    "#                                             model_type='llama')\n",
    "\n",
    "# pprint(llm('AI is going to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a341788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaContextQA:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            use_auth_token=True\n",
    "        )\n",
    "        self.context = None\n",
    "        self.history = []  # Stores list of {\"question\": ..., \"answer\": ..., \"timestamp\": ...}\n",
    "\n",
    "    def set_context(self, context, append=False):\n",
    "        \"\"\"Set or append to context.\"\"\"\n",
    "        if append and self.context:\n",
    "            self.context += \"\\n\" + context.strip()\n",
    "        else:\n",
    "            self.context = context.strip()\n",
    "\n",
    "    def clear_context(self):\n",
    "        \"\"\"Remove current context and history.\"\"\"\n",
    "        self.context = None\n",
    "        self.history = []\n",
    "\n",
    "    def ask_question(self, question, max_new_tokens=150):\n",
    "        \"\"\"Ask a question using current context and store history.\"\"\"\n",
    "        if not self.context:\n",
    "            return \"No context has been set.\"\n",
    "\n",
    "        prompt = f\"\"\"[INST] <<SYS>>\n",
    "            You are a helpful assistant.\n",
    "            <</SYS>>\n",
    "            Given the following content, answer the question concisely.\n",
    "\n",
    "            Content:\n",
    "            {self.context}\n",
    "\n",
    "            Question:\n",
    "            {question}\n",
    "            [/INST]\n",
    "            \"\"\"\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Clean up output\n",
    "        answer_start = response.find(\"Question:\")\n",
    "        if answer_start != -1:\n",
    "            answer = response[answer_start + len(\"Question:\"):].strip()\n",
    "        else:\n",
    "            answer = response.strip()\n",
    "\n",
    "        # Save to history\n",
    "        self.history.append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def save_session(self, filepath=\"qa_session.json\"):\n",
    "        \"\"\"Save context and Q&A history to a JSON file.\"\"\"\n",
    "        data = {\n",
    "            \"context\": self.context,\n",
    "            \"history\": self.history\n",
    "        }\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def load_session(self, filepath=\"qa_session.json\"):\n",
    "        \"\"\"Load context and Q&A history from a JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            self.context = data.get(\"context\", \"\")\n",
    "            self.history = data.get(\"history\", [])\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load session: {e}\")\n",
    "\n",
    "    def print_history(self):\n",
    "        \"\"\"Print Q&A history in a readable format.\"\"\"\n",
    "        if not self.history:\n",
    "            print(\"No question-answer history found.\")\n",
    "            return\n",
    "\n",
    "        print(\"Conversation History:\")\n",
    "        for i, entry in enumerate(self.history, 1):\n",
    "            print(f\"\\n Question {i} [{entry['timestamp']}]:\")\n",
    "            print(f\"   Question: {entry['question']}\")\n",
    "            print(f\"   Answer: {entry['answer']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b2e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = LlamaContextQA(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Set context\n",
    "qa.set_context(transcript)\n",
    "\n",
    "\n",
    "# def ask(question):\n",
    "#     user_prompt = f\"[INST] {system_prompt}\\n{question} [/INST]\"\n",
    "#     inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "\n",
    "while True:\n",
    "    question = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "    if question.lower() == 'exit':\n",
    "        break\n",
    "    answer = qa.ask_question(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "\n",
    "# Save session\n",
    "qa.save_session(\"Data/session.json\")\n",
    "\n",
    "# Clear and reload\n",
    "# qa.clear_context()\n",
    "# qa.load_session(\"session.json\")\n",
    "\n",
    "# Check reloaded history\n",
    "# for entry in qa.history:\n",
    "#     print(f\"[{entry['timestamp']}] Q: {entry['question']} ‚Üí A: {entry['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d79bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY Q&A HISTORY\n",
    "def display_history(qa, height=400):\n",
    "    \"\"\"Display Q&A history in the sidebar in a scrollable, expandable format.\"\"\"\n",
    "    with st.sidebar:\n",
    "        st.subheader(\"üìú Q&A History\")\n",
    "\n",
    "        if not qa.history:\n",
    "            st.info(\"üì≠ No history yet.\")\n",
    "            return\n",
    "\n",
    "        st.markdown(f\"<div style='height: {height}px; overflow-y: auto;'>\", unsafe_allow_html=True)\n",
    "\n",
    "        for i, entry in enumerate(qa.history, 1):\n",
    "            with st.expander(f\"üîπ Q{i} ‚Äî {entry['timestamp']}\"):\n",
    "                st.markdown(f\"**Q:** {entry['question']}\")\n",
    "                st.markdown(f\"**A:** {entry['answer']}\")\n",
    "\n",
    "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "\n",
    "# STREAMLIT UI\n",
    "st.set_page_config(page_title=\"ü¶ô LLaMA Contextual Q&A\", layout=\"centered\")\n",
    "st.title(\"ü¶ô LLaMA Contextual Q&A Assistant\")\n",
    "\n",
    "# Initialize session state\n",
    "if \"qa\" not in st.session_state:\n",
    "    st.session_state.qa = LlamaContextQA(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "qa = st.session_state.qa\n",
    "\n",
    "# Sidebar controls\n",
    "with st.sidebar:\n",
    "    st.header(\"‚öôÔ∏è Controls\")\n",
    "\n",
    "    # Save session with user-defined filename\n",
    "    save_filename = st.text_input(\"Save As:\", value=\"qa_session.json\")\n",
    "    if st.button(\"üíæ Save Session\"):\n",
    "        if save_filename:\n",
    "            try:\n",
    "                qa.save_session(save_filename)\n",
    "                st.success(f\"Session saved to `{save_filename}`.\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Failed to save: {e}\")\n",
    "        else:\n",
    "            st.warning(\"Please enter a filename.\")\n",
    "\n",
    "    # Load session from file uploader\n",
    "    uploaded_file = st.file_uploader(\"Load Session (.json)\", type=\"json\")\n",
    "    if uploaded_file is not None:\n",
    "        try:\n",
    "            qa.load_session(uploaded_file)\n",
    "            st.success(\"Session loaded from uploaded file.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Failed to load: {e}\")\n",
    "\n",
    "    # Clear session\n",
    "    if st.button(\"Clear Context & History\"):\n",
    "        qa.clear_context()\n",
    "        st.warning(\"Context and history cleared.\")\n",
    "\n",
    "# Set or update context\n",
    "with st.expander(\"Set or Update Context\"):\n",
    "    new_context = st.text_area(\"Enter context (you can update this at any time):\", height=150)\n",
    "    append = st.checkbox(\"Append to existing context\", value=False)\n",
    "    if st.button(\"Apply Context\"):\n",
    "        if new_context.strip():\n",
    "            qa.set_context(new_context, append=append)\n",
    "            st.success(\"üß† Context updated.\")\n",
    "        else:\n",
    "            st.warning(\"‚ö†Ô∏è Please enter some context.\")\n",
    "\n",
    "# Ask a question\n",
    "st.subheader(\"Ask a Question\")\n",
    "question = st.text_input(\"Type your question here:\")\n",
    "if st.button(\"Get Answer\"):\n",
    "    if not question.strip():\n",
    "        st.warning(\"Please enter a question.\")\n",
    "    else:\n",
    "        with st.spinner(\"üí¨ Thinking... generating answer...\"):\n",
    "            answer = qa.ask_question(question)\n",
    "            qa.save_session()  # Auto-save after each question\n",
    "        st.success(\"Answer:\")\n",
    "        st.write(answer)\n",
    "\n",
    "# Always show scrollable history in sidebar\n",
    "display_history(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c55eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d4d92e7e794687be06f0c40bb4a75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39425407f19d4dcea65b480114b0d654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c836fc479e4a8eb0560618cb7920fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d15fd5464ed4855b460d3b8f5768914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae23ee12e04d406f969f4e71809e6c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73f8075c4e44696bf3d9db2d2e4568c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Which models are mentioned as examples of deep learning in NLP?\n",
      "[/INST]\n",
      "The models mentioned as examples of deep learning in NLP are:\n",
      "\n",
      "1. Transformers\n",
      "2. BERT\n",
      "3. GPT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Choose your LLaMA model (must be downloaded from Hugging Face)    \n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"  # or another variant\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=True,\n",
    ")\n",
    "\n",
    "# Sample content and question\n",
    "context = \"\"\"\n",
    "Deep learning has revolutionized natural language processing by enabling models like transformers to learn contextual relationships in text. These models, including BERT and GPT, are pre-trained on large corpora and can be fine-tuned for various downstream tasks.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which models are mentioned as examples of deep learning in NLP?\"\n",
    "\n",
    "# Format prompt\n",
    "prompt = f\"\"\"[INST] <<SYS>>\n",
    "You are a helpful assistant.\n",
    "<</SYS>>\n",
    "Given the following content, answer the question concisely.\n",
    "\n",
    "Content:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Clean up response\n",
    "answer_start = response.find(\"Question:\")\n",
    "answer = response[answer_start + len(\"Question:\"):].strip()\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5a09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# # Define context (system message)\n",
    "# context = \"\"\"Deep learning has revolutionized natural language processing by enabling models like transformers to learn contextual relationships in text. These models, including BERT and GPT, are pre-trained on large corpora and can be fine-tuned for various downstream tasks.\"\"\"\n",
    "\n",
    "# # Format a reusable system prompt\n",
    "# system_prompt = f\"<<SYS>>\\nThe following is a context:\\n{context}\\n<</SYS>>\\n\"\n",
    "\n",
    "# # Track previous interaction (optional)\n",
    "# chat_history = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt}\n",
    "# ]\n",
    "\n",
    "# def ask(question):\n",
    "#     user_prompt = f\"[INST] {system_prompt}\\n{question} [/INST]\"\n",
    "#     inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     outputs = model.generate(**inputs, max_new_tokens=150)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "# # Example usage:\n",
    "# print(ask(\"Which models are mentioned?\"))\n",
    "# print(ask(\"What are those models trained on?\"))\n",
    "# print(ask(\"What task can they be fine-tuned for?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd11aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
